version: '3.8'

services:
  ## ← new Ollama service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"       # OpenAI-compatible endpoint
    volumes:
      - ollama_data:/root/.ollama

  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    
  api:
    build: .
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - QDRANT_URL=http://qdrant:6333
      # you can also override here instead of .env:
      # - OPENAI_BASE_URL=http://ollama:11434/v1
      # - OPENAI_API_KEY=dummy
    volumes:
      - ./logs:/app/logs
    depends_on:
      - qdrant
      - ollama   # make sure Ollama is up first

  viz:
    build: 
      context: .
      dockerfile: src/logging/viz/Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./logs:/app/logs
    depends_on:
      - api

volumes:
  qdrant_data:
  ollama_data:    # ← volume for Ollama’s pull-in models & cache
